{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Análisis de Datos de Sensores con Kafka y Spark Streaming\n",
"\n",
"Este notebook implementa un sistema de análisis en tiempo real de datos de sensores usando Apache Kafka y Spark Streaming.\n",
"El sistema procesa datos de temperatura y humedad de múltiples sensores, calculando promedios en ventanas temporales."
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 1. Configuración Inicial\n",
"\n",
"Importamos las bibliotecas necesarias y configuramos el entorno Spark."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"from pyspark.sql import SparkSession\n",
"from pyspark.sql.functions import from_json, col, window\n",
"from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType\n",
"import logging"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 2. Creación de la Sesión Spark\n",
"\n",
"Configuramos y creamos una sesión Spark con el nivel de logging adecuado."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"spark = SparkSession.builder \\n",
"    .appName("KafkaSparkStreaming") \\n",
"    .getOrCreate()\n",
"\n",
"spark.sparkContext.setLogLevel("WARN")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 3. Definición del Schema\n",
"\n",
"Definimos la estructura de los datos que recibiremos de los sensores:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"schema = StructType([\n",
"    StructField("sensor_id", IntegerType()),\n",
"    StructField("temperature", FloatType()),\n",
"    StructField("humidity", FloatType()),\n",
"    StructField("timestamp", TimestampType())\n",
"])"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 4. Configuración de la Lectura desde Kafka\n",
"\n",
"Establecemos la conexión con Kafka y configuramos la lectura del stream de datos."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"df = spark \\n",
"    .readStream \\n",
"    .format("kafka") \\n",
"    .option("kafka.bootstrap.servers", "localhost:9092") \\n",
"    .option("subscribe", "sensor_data") \\n",
"    .load()"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 5. Procesamiento de Datos\n",
"\n",
"Parseamos los mensajes JSON y realizamos las transformaciones necesarias."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"parsed_df = df.select(\n",
"    from_json(col("value").cast("string"), schema).alias("data")\n",
").select("data.*")\n",
"\n",
"windowed_stats = parsed_df \\n",
"    .groupBy(window(col("timestamp"), "1 minute"), "sensor_id") \\n",
"    .agg({"temperature": "avg", "humidity": "avg"})"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 6. Configuración de la Salida\n",
"\n",
"Configuramos la salida del streaming y iniciamos el procesamiento."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"query = windowed_stats \\n",
"    .writeStream \\n",
"    .outputMode("complete") \\n",
"    .format("console") \\n",
"    .start()\n",
"\n",
"query.awaitTermination()"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.8.0"
}
}
}